---
title: "Senior Project"
author: "Jared Ellison"
date: "2023-02-15"
output:
  word_document: default
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
fraud_oracle=read.csv("fraud_oracle.csv")
```


```{r}
library(dplyr)
library (ggplot2)
library (tidyr)
library("ROCR")
library(caret)
library(memisc)
library(MLmetrics)
library(boot)
library(randomForest)
```


# Data Cleaning

```{r}
actualfraud=subset(fraud_oracle, Age>0)
quantfraud = select_if(actualfraud, is.numeric)
catfraud = select_if(actualfraud, is.character)
catfraud$VehiclePrice2=factor(catfraud$VehiclePrice,
                              levels=levels(factor(catfraud$VehiclePrice))[c(5,1,2,3,4,6)])

actualfraud$VehiclePrice2=factor(actualfraud$VehiclePrice,
                              levels=levels(factor(catfraud$VehiclePrice))[c(5,1,2,3,4,6)])
tab = table(catfraud$Make)
otherlist = names(tab)[tab<=100]

fraud_oracle$Make2=fraud_oracle$Make
fraud_oracle$Make2[fraud_oracle$Make %in% otherlist] = "Other"
table(fraud_oracle$Make2)

actualfraud$AddrChng = actualfraud$AddressChange_Claim
actualfraud$AddrChng[actualfraud$AddrChng=="1 year"]="Under 1 year"
actualfraud$AddrChng[actualfraud$AddrChng=="under 6 months"]="Under 1 year"

counts = table (actualfraud$FraudFound_P)
wgts = 1/counts*1000
actualfraud$weight = wgts[1]
actualfraud$weight [actualfraud$FraudFound_P==1] = wgts[2]

catfraud1=catfraud[,c(1:2,4:5)]
catfraud2=catfraud[,c(6:10)]
catfraud3=catfraud[,c(11:15)]
catfraud4=catfraud[,c(16:20)]
catfraud5=catfraud[,c(21:25)]

library (ggplot2)
library (tidyr)

ggplot(gather(quantfraud), aes(value)) +
  geom_histogram(bins = 12) +
  facet_wrap(~key, scales = 'free')
ggplot(gather(catfraud1), aes(value)) +
  geom_bar() +
  facet_wrap(~key, scales = 'free') + 
  theme(axis.text.x = element_text(angle = 60, vjust = 1, hjust=1))
ggplot(gather(catfraud2), aes(value)) +
  geom_bar() +
  facet_wrap(~key, scales = 'free') + 
  theme(axis.text.x = element_text(angle = 60, vjust = 1, hjust=1))
ggplot(gather(catfraud3), aes(value)) +
  geom_bar() +
  facet_wrap(~key, scales = 'free') + 
  theme(axis.text.x = element_text(angle = 60, vjust = 1, hjust=1))
ggplot(gather(catfraud4), aes(value)) +
  geom_bar() +
  facet_wrap(~key, scales = 'free') + 
  theme(axis.text.x = element_text(angle = 60, vjust = 1, hjust=1))
ggplot(gather(catfraud5), aes(value)) +
  geom_bar() +
  facet_wrap(~key, scales = 'free') + 
  theme(axis.text.x = element_text(angle = 60, vjust = 1, hjust=1))
```

These are histograms of the data to look at counts and shapes of the quantitative variables. I made some changes to make of vehicle due to some low prevalence makes.

# First-Order Logistic Regression

```{r}
fit1=glm(FraudFound_P~.- Make - AddressChange_Claim - weight - VehiclePrice2 - PolicyNumber, family = binomial, data=actualfraud)
summary(fit1)
```

This model works fairly well, but didn't reduce deviance by too much.

```{r}
pred.logit1 = predict(fit1)
boxplot(pred.logit1~actualfraud$FraudFound_P, horizontal=T, outline = F, 
        ylim = c(min(pred.logit1),max(pred.logit1)))
fit1.ord = order (pred.logit1)
pred.pr = predict (fit1, type='response')
lines (pred.logit1 [fit1.ord], pred.pr [fit1.ord]+1, lwd=2, col='blue')
with(actualfraud, lines(lowess(FraudFound_P+1 ~ pred.logit1), lty=2, col='red'))

```

This predicted vs. actual plot shows that the claims that are actually fraudulent lie in the middle of the not fraudulent claims based on the logit values.

```{r}
actualfraud[pred.logit1>.6,]
```


```{r}
par(mfrow=c(1,2))
plot(fit1)
```

These residual plots aren't great, but they are ok.

```{r}
m1=step(fit1, k=log(15100), trace = 0)
summary(m1)
```

Stepwise regression with BIC.

```{r}
summary(m1)
```

Model after the BIC stepwise.

```{r}
library("ROCR")
## Warning: package 'ROCR' was built under R version 4.0.5
pred1 = prediction(m1$fitted.values, m1$y)
perf1 = performance(pred1,"tpr","fpr")
auc1 = performance(pred1,"auc")@y.values[[1]]
auc1
## [1] 0.9347378
plot(perf1, lwd=2, col=2)
abline(0,1)
legend(0.6, 0.3, c(paste ("AUC=", round (auc1, 4), sep="")), lwd=2, col=2)
roc.x = slot (perf1, "x.values") [[1]]
roc.y = slot (perf1, "y.values") [[1]]
cutoffs = slot (perf1, "alpha.values") [[1]]
auc.table = cbind.data.frame(cutoff=pred1@cutoffs,
tp=pred1@tp, fp=pred1@fp, tn=pred1@tn,
fn=pred1@fn)
names (auc.table) = c("Cutoff", "TP", "FP", "TN", "FN")
auc.table$sensitivity = auc.table$TP / (auc.table$TP + auc.table$FN)
auc.table$specificity = auc.table$TN / (auc.table$TN + auc.table$FP)
auc.table$FalsePosRate = 1 - auc.table$specificity
auc.table$sens_spec = auc.table$sensitivity + auc.table$specificity
auc.table$F1 = with(auc.table, 2*TP/(2*TP + FN + FP))
auc.best = auc.table [auc.table$sens_spec == max (auc.table$sens_spec),]
auc.best
F1.best = auc.table [auc.table$F1 == max (auc.table$F1),]
F1.best
points(auc.best$FalsePosRate, auc.best$sensitivity)
points(F1.best$FalsePosRate, F1.best$sensitivity, pch = 2)

plot(auc.table$sens_spec~auc.table$Cutoff)
abline(v=auc.best$Cutoff)

plot(auc.table$F1~auc.table$Cutoff)
abline(v=F1.best$Cutoff)

```

Graphs and values of cutoffs based on F1 score and sens_spec.

```{r}
preds = predict (m1, se.fit = T)
pred.df = cbind.data.frame (actualfraud, as.data.frame (preds))
pred.df$lwr = pred.df$fit - 1.96 * pred.df$se.fit
pred.df$upr = pred.df$fit + 1.96 * pred.df$se.fit
pred.df$fit.pr = round (exp (pred.df$fit) / (1 + exp (pred.df$fit)), 3)
pred.df$lwr.pr = round (exp (pred.df$lwr) / (1 + exp (pred.df$lwr)), 3)
pred.df$upr.pr = round (exp (pred.df$upr) / (1 + exp (pred.df$upr)), 3)

pred.df$pred.fail = ifelse (pred.df$fit.pr >= F1.best$Cutoff[1], "Pred.Yes",
"Pred.No")
table (actualfraud$FraudFound_P, pred.df$pred.fail)

```

Confusion matrix from the modle after stepwise regression.


# Second-Order Logistic Regression

```{r}
fit2=glm(formula = FraudFound_P ~ (AccidentArea + Age + Fault + PolicyType + 
    AddrChng)^2, family=binomial, data = actualfraud)
summary(fit2)
```

Better residual deviance, but still not the best.

```{r}
par(mfrow=c(1,2))
plot(fit2)
```

Large outlier in the residual plots.

```{r}
m2=step(fit2, k=log(15100), trace=0)
summary(m2)
```

BIC stepwise regression for the second-order model.

```{r}
pred.logit1 = predict(m2)
boxplot(pred.logit1~actualfraud$FraudFound_P, horizontal=T, outline = F, 
        ylim = c(min(pred.logit1),max(pred.logit1)))
fit1.ord = order (pred.logit1)
pred.pr = predict (m2, type='response')
lines (pred.logit1 [fit1.ord], pred.pr [fit1.ord]+1, lwd=2, col='blue')
with(actualfraud, lines(lowess(FraudFound_P+1 ~ pred.logit1), lty=2, col='red'))
```

Fitted vs. actual plot for the second-order logistic regression.

```{r}
par(mfrow=c(1,2))
plot(m2)
```

Residual plots for the second-order model after stepwise regression.

```{r}
library("ROCR")
## Warning: package 'ROCR' was built under R version 4.0.5
pred1 = prediction(m2$fitted.values, m2$y)
perf1 = performance(pred1,"tpr","fpr")
auc1 = performance(pred1,"auc")@y.values[[1]]
auc1
## [1] 0.9347378
plot(perf1, lwd=2, col=2)
abline(0,1)
legend(0.6, 0.3, c(paste ("AUC=", round (auc1, 4), sep="")), lwd=2, col=2)
roc.x = slot (perf1, "x.values") [[1]]
roc.y = slot (perf1, "y.values") [[1]]
cutoffs = slot (perf1, "alpha.values") [[1]]
auc.table = cbind.data.frame(cutoff=pred1@cutoffs,
tp=pred1@tp, fp=pred1@fp, tn=pred1@tn,
fn=pred1@fn)
names (auc.table) = c("Cutoff", "TP", "FP", "TN", "FN")
auc.table$sensitivity = auc.table$TP / (auc.table$TP + auc.table$FN)
auc.table$specificity = auc.table$TN / (auc.table$TN + auc.table$FP)
auc.table$FalsePosRate = 1 - auc.table$specificity
auc.table$sens_spec = auc.table$sensitivity + auc.table$specificity
auc.table$F1 = with(auc.table, 2*TP/(2*TP + FN + FP))
auc.best = auc.table [auc.table$sens_spec == max (auc.table$sens_spec),]
auc.best
F1.best = auc.table [auc.table$F1 == max (auc.table$F1),]
F1.best
points(auc.best$FalsePosRate, auc.best$sensitivity)
points(F1.best$FalsePosRate, F1.best$sensitivity, pch = 2)

plot(auc.table$sens_spec~auc.table$Cutoff)
abline(v=auc.best$Cutoff)

plot(auc.table$F1~auc.table$Cutoff)
abline(v=F1.best$Cutoff)

```

Graphs and values of cutoffs based on F1 score and sens_spec.

```{r}
preds = predict (m2, se.fit = T)
pred.df = cbind.data.frame (actualfraud, as.data.frame (preds))
pred.df$lwr = pred.df$fit - 1.96 * pred.df$se.fit
pred.df$upr = pred.df$fit + 1.96 * pred.df$se.fit
pred.df$fit.pr = round (exp (pred.df$fit) / (1 + exp (pred.df$fit)), 3)
pred.df$lwr.pr = round (exp (pred.df$lwr) / (1 + exp (pred.df$lwr)), 3)
pred.df$upr.pr = round (exp (pred.df$upr) / (1 + exp (pred.df$upr)), 3)

pred.df$pred.fail = ifelse (pred.df$fit.pr >= F1.best$Cutoff[1], "Pred.Yes",
"Pred.No")
table (actualfraud$FraudFound_P, pred.df$pred.fail)

```

Confusion matrix of the second-order model.

```{r}
pred.logit1 = predict(m2)
boxplot(pred.logit1~actualfraud$FraudFound_P, horizontal=T, outline = F, 
        ylim = c(min(pred.logit1),max(pred.logit1)))
fit1.ord = order (pred.logit1)
pred.pr = predict (m2, type='response')
lines (pred.logit1 [fit1.ord], pred.pr [fit1.ord]+1, lwd=2, col='blue')
with(actualfraud, lines(lowess(FraudFound_P+1 ~ pred.logit1), lty=2, col='red'))
```


# Removed Outliers from Logistic Regression

```{r}
fit4=glm(formula = FraudFound_P ~ (AccidentArea + Age + Fault + PolicyType + 
    AddrChng)^2, family=binomial, 
    data = actualfraud[!(row.names(actualfraud) %in% c("1","8008")),])
summary(fit4)
```

Fixed second order model without the outliers.

```{r}
par(mfrow=c(1,2))
plot(fit4)
```

Better residual plots with the outliers removed for the models.

```{r}
m3=step(fit4, k=log(15100), trace=0)
summary(m3)
```

Stepwise regression for the fixed second-order model.

```{r}
pred.logit1 = predict(m3)
boxplot(pred.logit1~actualfraud[!(row.names(actualfraud) %in% c("1","8008")),]$FraudFound_P, horizontal=T, outline = F, 
        ylim = c(min(pred.logit1),max(pred.logit1)))
fit1.ord = order (pred.logit1)
pred.pr = predict (m3, type='response')
lines (pred.logit1 [fit1.ord], pred.pr [fit1.ord]+1, lwd=2, col='blue')
with(actualfraud[!(row.names(actualfraud) %in% c("1","8008")),], lines(lowess(FraudFound_P+1 ~ pred.logit1), lty=2, col='red'))
```

Fitted vs. actual graph for the fixed second-order model.

# Cross-Validation for Logistic Regression

```{r}
library(caret)
library(memisc)
library(MLmetrics)
m2dv = dummyVars(~FraudFound_P + AccidentArea + Age + Fault + PolicyType + 
    AddrChng + Age:Fault + Fault:AddrChng, data=actualfraud, 
    fullRank = TRUE)
frauddv = data.frame(predict(m2dv, newdata=actualfraud))
m2check = glm(FraudFound_P ~ ., family = binomial, data=frauddv)
summary(m2check)

library(boot)
#m2cv = cv.glm(frauddv, m2check, cost = F1_Score, K=5)
#Didn't end up working
```


```{r}
library("ROCR")
## Warning: package 'ROCR' was built under R version 4.0.5
pred1 = prediction(m3$fitted.values, m3$y)
perf1 = performance(pred1,"tpr","fpr")
auc1 = performance(pred1,"auc")@y.values[[1]]
auc1
## [1] 0.9347378
plot(perf1, lwd=2, col=2)
abline(0,1)
legend(0.6, 0.3, c(paste ("AUC=", round (auc1, 4), sep="")), lwd=2, col=2)
roc.x = slot (perf1, "x.values") [[1]]
roc.y = slot (perf1, "y.values") [[1]]
cutoffs = slot (perf1, "alpha.values") [[1]]
auc.table = cbind.data.frame(cutoff=pred1@cutoffs,
tp=pred1@tp, fp=pred1@fp, tn=pred1@tn,
fn=pred1@fn)
names (auc.table) = c("Cutoff", "TP", "FP", "TN", "FN")
auc.table$sensitivity = auc.table$TP / (auc.table$TP + auc.table$FN)
auc.table$specificity = auc.table$TN / (auc.table$TN + auc.table$FP)
auc.table$FalsePosRate = 1 - auc.table$specificity
auc.table$sens_spec = auc.table$sensitivity + auc.table$specificity
auc.table$F1 = with(auc.table, 2*TP/(2*TP + FN + FP))
auc.best = auc.table [auc.table$sens_spec == max (auc.table$sens_spec),]
auc.best
F1.best = auc.table [auc.table$F1 == max (auc.table$F1),]
F1.best
points(auc.best$FalsePosRate, auc.best$sensitivity)
points(F1.best$FalsePosRate, F1.best$sensitivity, pch = 2)

plot(auc.table$sens_spec~auc.table$Cutoff)
abline(v=auc.best$Cutoff)

plot(auc.table$F1~auc.table$Cutoff)
abline(v=F1.best$Cutoff)

```

Graphs and values of cutoffs based on F1 score and sens_spec.

```{r}
preds = predict (m3, se.fit = T)
pred.df = cbind.data.frame (actualfraud[!(row.names(actualfraud) %in% c("1","8008")),], as.data.frame (preds))
pred.df$lwr = pred.df$fit - 1.96 * pred.df$se.fit
pred.df$upr = pred.df$fit + 1.96 * pred.df$se.fit
pred.df$fit.pr = round (exp (pred.df$fit) / (1 + exp (pred.df$fit)), 3)
pred.df$lwr.pr = round (exp (pred.df$lwr) / (1 + exp (pred.df$lwr)), 3)
pred.df$upr.pr = round (exp (pred.df$upr) / (1 + exp (pred.df$upr)), 3)

pred.df$pred.fail = ifelse (pred.df$fit.pr >= F1.best$Cutoff[1], "Pred.Yes",
"Pred.No")
table (actualfraud[!(row.names(actualfraud) %in% c("1","8008")),]$FraudFound_P, pred.df$pred.fail)

```

Confusion matrix for the fixed secon-order logistic regression.

```{r}
#Weighted Logistic Regression
#actualfraud$weight = ifelse(actualfraud$FraudFound_P==1, 15100/(2*892), 15100/(14208*2))
actualfraud$weight = ifelse(actualfraud$FraudFound_P==1, 1, 1)
fit3=glm(FraudFound_P~.-Make, family = quasibinomial, weights = weight,  data=actualfraud)
summary(fit3)
```


# Random Forest

```{r}
set.seed(42)
library(randomForest)
#actualfraud$rfFraud = ifelse(actualfraud$FraudFound_P, "Yes", "No")
rf1=randomForest(factor(FraudFound_P)~.-Make-AddressChange_Claim-weight-
                   PolicyNumber-VehiclePrice2,data=actualfraud)
print(rf1)
importance(rf1)
prob = predict(rf1, type="prob")
hist(rf1$oob.times)
```

Output from the first-order random forest model.

```{r}
library("ROCR")
## Warning: package 'ROCR' was built under R version 4.0.5
pred1 = prediction(prob[,2], rf1$y)
perf1 = performance(pred1,"tpr","fpr")
auc1 = performance(pred1,"auc")@y.values[[1]]
auc1
## [1] 0.9347378
plot(perf1, lwd=2, col=2)
abline(0,1)
legend(0.6, 0.3, c(paste ("AUC=", round (auc1, 4), sep="")), lwd=2, col=2)
roc.x = slot (perf1, "x.values") [[1]]
roc.y = slot (perf1, "y.values") [[1]]
cutoffs = slot (perf1, "alpha.values") [[1]]
auc.table = cbind.data.frame(cutoff=pred1@cutoffs,
tp=pred1@tp, fp=pred1@fp, tn=pred1@tn,
fn=pred1@fn)
names (auc.table) = c("Cutoff", "TP", "FP", "TN", "FN")
auc.table$sensitivity = auc.table$TP / (auc.table$TP + auc.table$FN)
auc.table$specificity = auc.table$TN / (auc.table$TN + auc.table$FP)
auc.table$FalsePosRate = 1 - auc.table$specificity
auc.table$sens_spec = auc.table$sensitivity + auc.table$specificity
auc.table$F1 = with(auc.table, 2*TP/(2*TP + FN + FP))
auc.best = auc.table [auc.table$sens_spec == max (auc.table$sens_spec),]
auc.best
F1.best = auc.table [auc.table$F1 == max (auc.table$F1),]
F1.best
points(auc.best$FalsePosRate, auc.best$sensitivity)
points(F1.best$FalsePosRate, F1.best$sensitivity, pch = 2)

plot(auc.table$sens_spec~auc.table$Cutoff)
abline(v=auc.best$Cutoff)

plot(auc.table$F1~auc.table$Cutoff)
abline(v=F1.best$Cutoff)

```

Graphs and values of cutoffs based on F1 score and sens_spec.

```{r}
table (actualfraud$FraudFound_P, prob[,2]>=F1.best$Cutoff)
```

Confusion matrix for the first-order random forest.

# Weighted Random Forest

```{r}
counts = table (actualfraud$FraudFound_P)
wgts = 1/counts*1000
actualfraud$weight = wgts[1]
actualfraud$weight [actualfraud$FraudFound_P==1] = wgts[2]

rf2=randomForest(factor(FraudFound_P)~.-Make-AddressChange_Claim-weight-
                   PolicyNumber-VehiclePrice2,data=actualfraud, classwt=wgts)
print(rf2)
importance(rf2)
prob = predict(rf2, type="prob")

plot(rf2$err.rate[,1])
```

Output from the weighted random forest model and the error rates from number of trees.

```{r}
library("ROCR")
## Warning: package 'ROCR' was built under R version 4.0.5
pred1 = prediction(prob[,2], rf2$y)
perf1 = performance(pred1,"tpr","fpr")
auc1 = performance(pred1,"auc")@y.values[[1]]
auc1
## [1] 0.9347378
plot(perf1, lwd=2, col=2)
abline(0,1)
legend(0.6, 0.3, c(paste ("AUC=", round (auc1, 4), sep="")), lwd=2, col=2)
roc.x = slot (perf1, "x.values") [[1]]
roc.y = slot (perf1, "y.values") [[1]]
cutoffs = slot (perf1, "alpha.values") [[1]]
auc.table = cbind.data.frame(cutoff=pred1@cutoffs,
tp=pred1@tp, fp=pred1@fp, tn=pred1@tn,
fn=pred1@fn)
names (auc.table) = c("Cutoff", "TP", "FP", "TN", "FN")
auc.table$sensitivity = auc.table$TP / (auc.table$TP + auc.table$FN)
auc.table$specificity = auc.table$TN / (auc.table$TN + auc.table$FP)
auc.table$FalsePosRate = 1 - auc.table$specificity
auc.table$sens_spec = auc.table$sensitivity + auc.table$specificity
auc.table$F1 = with(auc.table, 2*TP/(2*TP + FN + FP))
auc.best = auc.table [auc.table$sens_spec == max (auc.table$sens_spec),]
auc.best
F1.best = auc.table [auc.table$F1 == max (auc.table$F1),]
F1.best
points(auc.best$FalsePosRate, auc.best$sensitivity)
points(F1.best$FalsePosRate, F1.best$sensitivity, pch = 2)

plot(auc.table$sens_spec~auc.table$Cutoff)
abline(v=auc.best$Cutoff)

plot(auc.table$F1~auc.table$Cutoff)
abline(v=F1.best$Cutoff)


```

Graphs and values of cutoffs based on F1 score and sens_spec.

```{r}
table (actualfraud$FraudFound_P, prob[,2]>=F1.best$Cutoff)
```

Confusion matrix for the weighted random forest model.

```{r}
test.tune <- tuneRF(actualfraud[,-which(names(actualfraud) %in% 
                                  c("Make", "AddrChng", "weight", "VehiclePrice2", "FraudFound_P"))], factor(actualfraud$FraudFound_P), stepFactor=1.5)
```

# Tuning a Random Forest

```{r}
#mt = MTry, ns. =nodeSize, mn = maxNodes
myRF = function(mt, ns, mn){
  RFTry = randomForest(factor(FraudFound_P)~.-Make-AddressChange_Claim-weight-
                         PolicyNumber-VehiclePrice2,
                 data=actualfraud, mtry=mt, nodesize=ns, maxnodes=mn, ntree = 75)
  #Find the optimal cutoff for the confusion matrix
  prob = predict(RFTry, type="prob")
  pred1 = prediction(prob[,2], RFTry$y)
  perf1 = performance(pred1,"tpr","fpr")
  auc.table = cbind.data.frame(cutoff=pred1@cutoffs,
                               tp = pred1@tp, fp = pred1@fp, tn = pred1@tn,
                               fn = pred1@fn)
  names (auc.table) = c("Cutoff", "TP", "FP", "TN", "FN")
  auc.table$F1 = with(auc.table, 2 * TP / (2 * TP + FN + FP))
  F1.best = auc.table [auc.table$F1 == max (auc.table$F1), ]
  return(list(F1 = F1.best$F1 ))
}
#myRF(7, 5, 2^10)

```

R function created to run a random forest with different tuning parameters.

```{r}
#5 vectors of length 27
mtvec = numeric(27)
nsvec = numeric(27)
mnvec = numeric(27)
errorvec = numeric(27)
f1vec = numeric(27)

rowcount = 1

for (mt in c(10, 20, 30) ) {
  for(ns in c(5,10,15)){
    for (mn in c(2^7, 2^10, 2^13)){
      my.rf.res = myRF(mt, ns, mn)
      mtvec[rowcount] = mt
      nsvec[rowcount] = ns
      mnvec[rowcount] = mn
      f1vec[rowcount] = my.rf.res$F1
      rowcount = rowcount+1
    }
  }
}

results = data.frame(mtvec, nsvec, mnvec, errorvec, f1vec)
results
```

Triple for loop saving the inputs and outputs into a dataframe to use a response surface model to maximize F1 score.

```{r}
library(rsm)
results$log2mn = log(results$mnvec,base = 2)
rsm1 = rsm(f1vec~SO(mtvec, nsvec, log2mn),data = results)
summary(rsm1)
```

Output from the response surface model.

```{r}
contour(rsm1, ~mtvec + log2mn, image = T)
```

Contour plot showing the best mtry and max nodes values to maximize F1 score.

# Tuned Random Forest Model

```{r}
rf3 = randomForest(factor(FraudFound_P)~.-Make-AddressChange_Claim-weight-
                   PolicyNumber-VehiclePrice2,data=actualfraud, ntree = 75, mtry = 30, maxnodes = 2^10)
print(rf3)
importance(rf3)
prob = predict(rf3, type="prob")
```

Output and importance from the tuned random forest model.

```{r}
library("ROCR")
## Warning: package 'ROCR' was built under R version 4.0.5
pred1 = prediction(prob[,2], rf3$y)
perf1 = performance(pred1,"tpr","fpr")
auc1 = performance(pred1,"auc")@y.values[[1]]
auc1
## [1] 0.9347378
plot(perf1, lwd=2, col=2)
abline(0,1)
legend(0.6, 0.3, c(paste ("AUC=", round (auc1, 4), sep="")), lwd=2, col=2)
roc.x = slot (perf1, "x.values") [[1]]
roc.y = slot (perf1, "y.values") [[1]]
cutoffs = slot (perf1, "alpha.values") [[1]]
auc.table = cbind.data.frame(cutoff=pred1@cutoffs,
tp=pred1@tp, fp=pred1@fp, tn=pred1@tn,
fn=pred1@fn)
names (auc.table) = c("Cutoff", "TP", "FP", "TN", "FN")
auc.table$sensitivity = auc.table$TP / (auc.table$TP + auc.table$FN)
auc.table$specificity = auc.table$TN / (auc.table$TN + auc.table$FP)
auc.table$FalsePosRate = 1 - auc.table$specificity
auc.table$sens_spec = auc.table$sensitivity + auc.table$specificity
auc.table$F1 = with(auc.table, 2*TP/(2*TP + FN + FP))
auc.best = auc.table [auc.table$sens_spec == max (auc.table$sens_spec),]
auc.best
F1.best = auc.table [auc.table$F1 == max (auc.table$F1),]
F1.best
points(auc.best$FalsePosRate, auc.best$sensitivity)
points(F1.best$FalsePosRate, F1.best$sensitivity, pch = 2)

plot(auc.table$sens_spec~auc.table$Cutoff)
abline(v=auc.best$Cutoff)

plot(auc.table$F1~auc.table$Cutoff)
abline(v=F1.best$Cutoff)
```

Graphs and values of cutoffs based on F1 score and sens_spec.

```{r}
table (actualfraud$FraudFound_P, prob[,2]>F1.best$Cutoff)
```

Confusion matrix for the tuned random forest model.

# Best Random Forest Graph

```{r}
# Observed vs. Predicted for random forest number 1
pred.prob1 = predict(rf1,type = "prob")[,2]
pred.prob1[pred.prob1==0]=.0022
pred.logit1 = log(pred.prob1/(1-pred.prob1))
boxplot(pred.logit1~actualfraud$FraudFound_P, horizontal=T, outline = F, 
        ylim = c(min(pred.logit1),max(pred.logit1)))
points(jitter(actualfraud$FraudFound_P, .25)+1 ~ pred.logit1)
fit1.ord = order (pred.logit1)
lines (pred.logit1 [fit1.ord], pred.prob1 [fit1.ord]+1, lwd=2, col='blue')
with(actualfraud, lines(lowess(FraudFound_P+1 ~ pred.logit1), lty=2, col='red'))
```

Fitted vs. actual plot for the best random forest(First-order model) based on F1 score.

# Variable Importance Plot for Best Random Forest

```{r}
impdf = data.frame (rf1$importance)
impdf$varname = rownames (rf1$importance)
impdf %>% ggplot (aes(y=reorder (varname, MeanDecreaseGini), 
                      x=MeanDecreaseGini)) + 
  geom_col()
```


